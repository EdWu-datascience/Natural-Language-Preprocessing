{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "natural language preprocessing_text preprocessing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOTxbuV4ZO14E6cauVbfkE4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdWu-datascience/natural-language-proprecessing/blob/main/natural_language_preprocessing_text_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwEdb9y81s6V"
      },
      "source": [
        "**this is only used for natural language preprocessing,more precisely,it's \n",
        "about text preprocessing**\n",
        "\n",
        "we'll use NLTK tool for text preprocessing and the dataset that we are going to use is Twitter dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ikPUDaswJ6i",
        "outputId": "f26cc309-2006-4e60-dccf-1b54f4d50813"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import twitter_samples\n",
        "nltk.download(\"twitter_samples\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFIIaW-V6lpq",
        "outputId": "8339c933-33a1-45db-d050-f94df96b8286"
      },
      "source": [
        "print(twitter_samples)\n",
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "print(twitter_samples.strings(\"positive_tweets.json\")[0:10])\n",
        "print(twitter_samples.strings('negative_tweets.json')[0:10])\n",
        "print('the type of postive_tweets is:',type(positive_tweets))\n",
        "print('the type of negative_tweets is:',type(negative_tweets))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<TwitterCorpusReader in '/root/nltk_data/corpora/twitter_samples'>\n",
            "['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)', '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!', '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!', '@97sides CONGRATS :)', 'yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days', '@BhaktisBanter @PallaviRuhail This one is irresistible :)\\n#FlipkartFashionFriday http://t.co/EbZ0L2VENM', \"We don't like to keep our lovely customers waiting for long! We hope you enjoy! Happy Friday! - LWWF :) https://t.co/smyYriipxI\", '@Impatientraider On second thought, there’s just not enough time for a DD :) But new shorts entering system. Sheep must be buying.', 'Jgh , but we have to go to Bayan :D bye', 'As an act of mischievousness, am calling the ETL layer of our in-house warehousing app Katamari.\\n\\nWell… as the name implies :p.']\n",
            "['hopeless for tmr :(', \"Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\", '@Hegelbon That heart sliding into the waste basket. :(', '“@ketchBurning: I hate Japanese call him \"bani\" :( :(”\\n\\nMe too', 'Dang starting next week I have \"work\" :(', \"oh god, my babies' faces :( https://t.co/9fcwGvaki0\", '@RileyMcDonough make me smile :((', '@f0ggstar @stuartthull work neighbour on motors. Asked why and he said hates the updates on search :( http://t.co/XvmTUikWln', 'why?:(\"@tahuodyy: sialan:( https://t.co/Hv1i0xcrL2\"', 'Athabasca glacier was there in #1948 :-( #athabasca #glacier #jasper #jaspernationalpark #alberta #explorealberta #… http://t.co/dZZdqmf7Cz']\n",
            "the type of postive_tweets is: <class 'list'>\n",
            "the type of negative_tweets is: <class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHW1tgn77yiW"
      },
      "source": [
        "**lets take a look at the number of pos and neg tweets for our twitter_samples dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wrs6D8sv7-kC",
        "outputId": "57e12c76-ccc1-4dc5-b64a-09e2861b470b"
      },
      "source": [
        "print('the length of positive tweet is:',len(positive_tweets))\n",
        "print('the length of negative tweet is:',len(negative_tweets))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the length of positive tweet is: 5000\n",
            "the length of negative tweet is: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_m6O7pAR9Ax8",
        "outputId": "88a09ca1-ed40-4af1-f7ad-986386e53468"
      },
      "source": [
        "import random\n",
        "random.seed(1)\n",
        "index_pos=random.randint(0,5000)\n",
        "index_neg=random.randint(0,5000)\n",
        "print(index_pos)\n",
        "print(index_neg)\n",
        "print('the positive tweet example is \\n',positive_tweets[index_pos])\n",
        "print('the negative tweet example is \\n',negative_tweets[index_neg])\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1100\n",
            "4662\n",
            "the positive tweet example is \n",
            " @jellybrations well thanks guys, we will be sure to do all of those things! Hope you're ok :)\n",
            "the negative tweet example is \n",
            " Longmorn 30 y.o. supposed to be a replacement to Tobermory 32 y.o. but... Beginning is strong, tail is missing :( http://t.co/NZ8zUhrA3F\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Kbl7Dt3-jpL"
      },
      "source": [
        "**preprocessing text data**\n",
        "\n",
        "\n",
        "1.   eleminate handles and urls\n",
        "2.   tokenize the string into words\n",
        "\n",
        "1.   remove stop words like \"and,is,a,etc\" and punctuation\n",
        "2.   stemming or convert every word to its stem\n",
        "\n",
        "1.   convert all words into lower case.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25D-Uxd5FOAJ"
      },
      "source": [
        "1:eleminate handles and urls"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YEcrXviD8hK"
      },
      "source": [
        "positive_sample = positive_tweets[index_pos]\n",
        "negative_sample = negative_tweets[index_neg]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvimnBdI-eDL",
        "outputId": "e916ac44-296b-45b2-e47a-626adf367203"
      },
      "source": [
        "print(positive_sample)\n",
        "print('\\n')\n",
        "print(negative_sample)\n",
        "print('\\n')\n",
        "import re \n",
        "#remove url\n",
        "negative_sample = re.sub(r'https?:\\/\\/.*[\\r\\n]*','',negative_sample)\n",
        "# remove the '@' letter from string\n",
        "positive_sample = re.sub(r'@','',positive_sample)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@jellybrations well thanks guys, we will be sure to do all of those things! Hope you're ok :)\n",
            "\n",
            "\n",
            "Longmorn 30 y.o. supposed to be a replacement to Tobermory 32 y.o. but... Beginning is strong, tail is missing :( http://t.co/NZ8zUhrA3F\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZLrrgwMFGIH",
        "outputId": "0e3080f1-99bf-439d-85cb-fb4a1a57b95f"
      },
      "source": [
        "print(positive_sample)\n",
        "print(negative_sample)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jellybrations well thanks guys, we will be sure to do all of those things! Hope you're ok :)\n",
            "Longmorn 30 y.o. supposed to be a replacement to Tobermory 32 y.o. but... Beginning is strong, tail is missing :( \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdGUj-K4FSxq"
      },
      "source": [
        "2:tokenize string into words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2ruM-1YINrB",
        "outputId": "488ee977-aa6a-47a2-c9f4-46450faeec31"
      },
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "toke = TweetTokenizer()\n",
        "positive_samples = toke.tokenize(positive_sample)\n",
        "negative_samples = toke.tokenize(negative_sample)\n",
        "print(positive_samples)\n",
        "print(negative_samples)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['jellybrations', 'well', 'thanks', 'guys', ',', 'we', 'will', 'be', 'sure', 'to', 'do', 'all', 'of', 'those', 'things', '!', 'Hope', \"you're\", 'ok', ':)']\n",
            "['Longmorn', '30', 'y', '.', 'o', '.', 'supposed', 'to', 'be', 'a', 'replacement', 'to', 'Tobermory', '32', 'y', '.', 'o', '.', 'but', '...', 'Beginning', 'is', 'strong', ',', 'tail', 'is', 'missing', ':(']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7XhORImI9HM"
      },
      "source": [
        "3:remove stopwords from tweet\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7texBvAJW1K",
        "outputId": "d2decc57-2890-4b91-ba9f-bccdf492e615"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOV0t0onJBYI",
        "outputId": "d2eae46b-a89f-45e8-b133-95ab5e37e3fe"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "#lets take a look at stop word\n",
        "print(stopwords.words('english')[0:100])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvhOGGrzMuDC"
      },
      "source": [
        "stopword = list(stopwords.words('english'))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-5nReURJdhI",
        "outputId": "dcc5abf3-656b-4a53-ae95-30b5f2ab18be"
      },
      "source": [
        "pos_filtered = []\n",
        "neg_filtered = []\n",
        "for L in positive_samples:\n",
        "  if L not in stopword:\n",
        "    pos_filtered.append(L)\n",
        "for L in negative_samples:\n",
        "  if L not in stopword:\n",
        "    neg_filtered.append(L)\n",
        "print(pos_filtered)\n",
        "print(neg_filtered)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['jellybrations', 'well', 'thanks', 'guys', ',', 'sure', 'things', '!', 'Hope', 'ok', ':)']\n",
            "['Longmorn', '30', '.', '.', 'supposed', 'replacement', 'Tobermory', '32', '.', '.', '...', 'Beginning', 'strong', ',', 'tail', 'missing', ':(']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ullIzIlcyQ0a"
      },
      "source": [
        "3:remove punctuation from tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF5QgyTaNIlw",
        "outputId": "6ddd73f7-9b2c-49ac-a1cb-999156375bb8"
      },
      "source": [
        "import string \n",
        "pos_pun = []\n",
        "neg_pun = []\n",
        "punctuation=string.punctuation\n",
        "for P in pos_filtered:\n",
        "  if P not in punctuation:\n",
        "    pos_pun.append(P)\n",
        "for P in neg_filtered:\n",
        "  if P not in punctuation:\n",
        "    neg_pun.append(P)\n",
        "print(pos_pun)\n",
        "print(neg_pun)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['jellybrations', 'well', 'thanks', 'guys', 'sure', 'things', 'Hope', 'ok', ':)']\n",
            "['Longmorn', '30', 'supposed', 'replacement', 'Tobermory', '32', '...', 'Beginning', 'strong', 'tail', 'missing', ':(']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sfFh38AzWom"
      },
      "source": [
        "4:stemming or convert every word into its stem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBYC5sX2zGsa",
        "outputId": "f3fe878c-24c0-4682-bc79-36908531fc97"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "STEM = PorterStemmer()\n",
        "pos_processed = []\n",
        "neg_processed = []\n",
        "for L in pos_pun:\n",
        "  word = STEM.stem(L)\n",
        "  pos_processed.append(word)\n",
        "for L in neg_pun:\n",
        "  word = STEM.stem(L)\n",
        "  neg_processed.append(word)\n",
        "print(pos_processed)\n",
        "print(neg_processed)\n",
        "print(positive_tweets[index_pos])\n",
        "print(negative_tweets[index_neg])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['jellybr', 'well', 'thank', 'guy', 'sure', 'thing', 'hope', 'ok', ':)']\n",
            "['longmorn', '30', 'suppos', 'replac', 'tobermori', '32', '...', 'begin', 'strong', 'tail', 'miss', ':(']\n",
            "@jellybrations well thanks guys, we will be sure to do all of those things! Hope you're ok :)\n",
            "Longmorn 30 y.o. supposed to be a replacement to Tobermory 32 y.o. but... Beginning is strong, tail is missing :( http://t.co/NZ8zUhrA3F\n"
          ]
        }
      ]
    }
  ]
}